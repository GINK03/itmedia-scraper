　今、IT業界を中心に大きな注目を集めているキーワードが「ビッグデータ」だ。特に今年の夏ごろからベンダ企業がビッグデータを掲げた製品やサービスを相次いで発表するなど、しばらくこのトレンドは続くとみられている。 　そもそも、ビッグデータとは何か。野村総合研究所のICT・メディア産業コンサルティング部で主任コンサルタントを務める鈴木良介氏は、「事業に役立つ知見を導出するための“高解像”“高頻度生成”“非構造なものを含む多様”なデータ」と定義する。単に巨大なデータということではなく、経営者視点に立ち、事業への付加価値を生むようなデータ特性を想定すると、結果として巨大（ビッグ）になるということである。 　実はビッグデータという言葉は以前から存在するものであり、鈴木氏によると、1997年に米SGI（Silicon Graphics, Inc）のチーフサイエンティストであるJohn R. Mashey氏が最初に使ったとされている。では、なぜこの数年間で急激にビッグデータに対する関心が高まっているのか。鈴木氏は3つの理由を挙げて説明する。 　1つ目は、「Suica」をはじめとするICカード、GPSを標準搭載したモバイル端末、ポータブル音楽プレイヤー「iPod」などの登場によって電子化・自動化が進展し、おのずと活用できるデータがたまってきたためである。2つ目は、ITによる業務の電子化や自動化はほぼ完了し、他社と差別化する上でデータから事業に寄与する知見を導き出せるかというニーズが高まったためである。3つ目は、ビッグデータを取得・生成、蓄積、処理・分析するためのツールが成熟してきたためである。「クラウドコンピューティングやオープンソースの分散処理技術であるHadoopなどによって、大量データを低コストで手間をかけずに処理できるようになった」と鈴木氏は述べる。 　しかし、ビッグデータを企業が活用する上で課題も存在する。中でも大きな課題の1つが人材不足である。実際、シリコンバレーにおいてもデータ解析スキルを持つ人材の争奪戦が始まっているなど、ビッグデータの取り扱いに関するリテラシーを持つ人材を多くの企業が渇望している。さらに日本においては、データ分析に対価を払う文化がないため、今後は自社のデータ分析を外注するなどの文化醸成も必要だとしている。 ※本記事は、2011年12月1日に行われた「第166回 NRIメディアフォーラム」を基に作成。 Copyright© 2017 ITmedia, Inc. All Rights Reserved.